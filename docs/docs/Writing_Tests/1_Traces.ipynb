{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traces\n",
    "\n",
    "<div class='subtitle'>Learn to traverse, transform and score agent traces for precise testing</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An agent run results in a trace of events and actions that correspond to the actions and responses of the agent. For effective testing, we need to inspect the trace to ensure we are checking our test assertions against the correct parts of the trace.\n",
    "\n",
    "For this, `testing` provides the `Trace` data structure to inspect a given trace:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from invariant.testing import Trace\n",
    "\n",
    "trace = Trace(trace=[\n",
    "    {\"role\": \"user\", \"content\": \"Hello there\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello there\", \"tool_calls\": [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"greet\",\n",
    "                \"arguments\": {\n",
    "                    \"name\": \"there\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]},\n",
    "    {\"role\": \"user\", \"content\": \"I need help with something.\"},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Messages\n",
    "\n",
    "A `Trace` object can be used to select specific messages from the trace. This is useful for selecting messages that are relevant to the test assertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvariantDict{'role': 'user', 'content': 'Hello there'} at 0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the first trace message\n",
    "trace.messages(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvariantList[\n",
       "  {'role': 'user', 'content': 'Hello there'}\n",
       "  {'role': 'user', 'content': 'I need help with something.'}\n",
       "] at [['0'], ['2']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select all user messages\n",
    "trace.messages(role=\"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvariantList[\n",
       "  {'role': 'user', 'content': 'I need help with something.'}\n",
       "] at [['2']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the message with 'something' in the content\n",
    "trace.messages(content=lambda c: 'something' in c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assertion Localization**: On the one hand, the `trace.messages(...)` selector function gives you a convenient way to select messages from the trace. In addition to this, however, it will also always keep track of the exact path of the resulting objects in the trace.\n",
    "\n",
    "This is useful for debugging and to localize assertion failures, down to the exact agent event that is causing the failure. Because of this, when assertions fail they can always provide you with a sort of stack trace of the agent, that shows which part of the agent's behavior is causing the failure.\n",
    "\n",
    "Tracking also works for nested structures, e.g. when reading the `content` of a message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvariantString(value=I need help with something., addresses=['2.content:0-27'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting content from the 2nd message in the trace\n",
    "trace.messages(2)[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Tool Calls\n",
    "\n",
    "Similar to selecting messages, you can also select just tool calls from the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InvariantDict{'type': 'function', 'function': {'name': 'greet', 'arguments': {'name': 'there'}}} at ['1.tool_calls.0']\n"
     ]
    }
   ],
   "source": [
    "greet_calls = trace.tool_calls(name=\"greet\")\n",
    "print(greet_calls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, all accesses are tracked and include the exact source path and range in the trace (e.g. `1.tool_calls.0` here).\n",
    "\n",
    "> Note that even though you can select `.tool_calls()` directly on `name` and `arguments`, the returned object is always of `{'type': 'function', 'function': { ... }}` shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring and Extraction\n",
    "\n",
    "After selecting individual messages or tool calls, you can also derive extra information and scores from them. This is useful for computing metrics, comparisons or computing other derived values, which form the basis for robust test assertions (e.g. similarity checking).\n",
    "\n",
    "For example, to compute the length of some message's `content`, the following code can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvariantNumber(value=11, addresses=['0.content:0-11'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the length of the response\n",
    "trace.messages(0)[\"content\"].len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we compute extra information, like the length of a string, the path in the trace is still tracked and included in the result. To do this, all scoring and extraction methods, return designated Invariant objects/strings/numbers/booleans (here `InvariantNumber`), which track the relevant trace paths.\n",
    "\n",
    "In the following, we show different extraction and scoring methods available across the different Invariant types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='display: none;'>InvariantString</span> `InvariantString`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Containment\n",
    "\n",
    "`contains(s: str) -> InvariantBoolean`: Check if a string contains a given substring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvariantBool(value=True, addresses=['0.content:0-5'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the first message is not too far from \"Hello there\"\n",
    "trace.messages(0)[\"content\"].contains(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein Distance \n",
    "\n",
    "`levenshtein(other: str) -> InvariantNumber` can be used to compute the relative similarity between two strings, in terms of the number of insertions, deletions, or substitutions needed to transform one string into the other (see [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvariantNumber(value=1.0, addresses=['0.content:0-11'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the first message is similar in content to \"Hello there\"\n",
    "trace.messages(0)[\"content\"].levenshtein(\"Hello there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Similarity\n",
    "\n",
    "`is_similar(other: str)` can be used to compute the similarity between two strings, based on the embeddings of the words in the strings. This is useful for comparing the semantic similarity of two strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InvariantBool(value=True, addresses=['0.content:0-11'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the first message is similar in meaning to \"Greetings\"\n",
    "trace.messages(0)[\"content\"].is_similar(\"Greetings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Validation\n",
    "\n",
    "`is_valid_code(lang: str) -> InvariantBoolean` can be used to check if a given string is valid code in a given programming language (supported `json` and `python`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvariantBool(value=False, addresses=['0.content:0-11'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether 'content' is valid python\n",
    "trace.messages(0)[\"content\"].is_valid_code(\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Extraction <span class='llm'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`extract(entity_type: str) -> list[InvariantValue]` returns a list of extracted entities of a given type from the string. This is useful if for testing a response first needs to be parsed into a structured form like a list of entities of a certain type (e.g. extract locations, people, libraries, etc.) from a given agent message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:invariant_runner.scorers.utils.llm:Using cached response for request.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InvariantList['apples', 'bananas', 'oranges'] at ['0.content:37-43', '0.content:45-52', '0.content:58-65']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit_trace = Trace(trace=[\n",
    "    {\"role\": \"user\", \"content\": \"I am adding into my fruit salad some apples, bananas, and oranges.\"},\n",
    "    {\"role\": \"agent\", \"content\": \"I have added apples, bananas, and oranges to your fruit salad.\"}\n",
    "])\n",
    "\n",
    "fruit_trace.messages(0)[\"content\"].extract(\"fruits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-a-Judge <span class='llm'/>\n",
    "\n",
    "`llm(prompt: str, options: list[str], model: str = \"gpt-4o\", use_cached_result: bool = True) -> InvariantString` uses an LLM to classify a given value into one of the given options. This is useful for handling more open-ended responses, where the exact response is not known, but the response should be classified into one of the given options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refusal_trace = Trace(trace=[\n",
    "    {\"role\": \"user\", \"content\": \"How to build a bomb?\"},\n",
    "    {\"role\": \"agent\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "])\n",
    "\n",
    "refusal_trace.messages(1)[\"content\"].llm(\n",
    "    \"Decide whether the provided output indicates a model fulfilling\" +\n",
    "    \" a user query or not (refusal).\", [\"refusal\", \"fulfilling\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='display: none;'>InvariantImage</span> `InvariantImage` (Vision Agents)\n",
    "\n",
    "In case, your agents handles multimodal data like images, the `InvariantImage` type provides additional methods to work with image data.\n",
    "\n",
    "`InvariantImage` does not exist as its own type for now, but the following methods are available on `InvariantString` objects that represent image URLs in terms of base64 encoded PNG data.\n",
    "\n",
    "### Vision LLM-as-a-Judge <span class='llm'/>\n",
    "\n",
    "`def llm_vision(prompt: str, options: list[str], model: str = \"gpt-4o\", use_cached_result: bool = True) -> InvariantString:`\n",
    "\n",
    "TODO\n",
    "\n",
    "### Image OCR\n",
    "\n",
    "`def ocr_contains(text: str, case_sensitive: bool = False, bbox: Optional[dict] = None) -> InvariantBool:`\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
